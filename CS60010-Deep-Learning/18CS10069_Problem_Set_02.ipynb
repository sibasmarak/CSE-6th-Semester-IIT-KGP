{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvnm3YqXF1et"
   },
   "source": [
    "\n",
    "# Problem Set 2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pK-9wPs4F1e1"
   },
   "source": [
    "**Name**: Siba Smarak Panigrahi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWcIfKAnF1e3"
   },
   "source": [
    "**Roll-No**: 18CS10069 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8ITxsdeF1e4"
   },
   "source": [
    "**Dept**: Computer Science and Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-IFdWvrF1e5"
   },
   "source": [
    "**Mail-ID**: sibasmarak.p@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idj_n4-4F1e5"
   },
   "source": [
    "## Preamble\n",
    "\n",
    "To run and solve this assignment, one must have a working IPython Notebook installation. The easiest way to set it up for both Windows and Linux is to install [Anaconda](https://docs.anaconda.com/anaconda/install/). Then save this file to your computer, run Anaconda and choose this file in Anaconda's file explorer. Use `Python 3` version. Below statements assume that you have already followed these instructions. If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/).\n",
    "\n",
    "Put your solution into boxes marked with **`[double click here to add a solution]`** and press Ctr+Enter to render text. None of the parts of this assignment require use of a machine with a GPU. You may complete the assignment using your local machine or you may use Google Colaboratory.\n",
    "\n",
    "We highly encourage students to put down their answers to theoretical questions into corresponding cells below. However, if one does not know LaTeX (and would find it too hard to learn it), he/she can write it in pen-and-paper format and submit the scanned pdf. Note that the solutions to the programming problems should be submitted in the ipynb file itself.\n",
    "\n",
    "Submission instructions: Please upload your completed solution file to [KGP Moodle](https://kgpmoodle.iitkgp.ac.in/moodle/login/index.php) by the due date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgHLWu4Iq0sH"
   },
   "source": [
    "### Problem 1: Gradient Descent Update Rule\n",
    "\n",
    "**Q1.1**: In learning neural networks, we typically minimize a loss function $\\mathcal{L}(w)$ with respect to the network parameters $w$. It is also important that we *regularize* the network to reduce overfitting. A simple and popular regularization strategy is to penalize some *norm* of $w$.\n",
    "\n",
    "Consider that we have $N$ examples $(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)$ such that $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\{-1, 1\\}, i = 1...N$. Also consider that we have at our disposal a single neuron. Let $w = [w_1, w_2, ..., w_d]^T$ be the weight vector and the output be given by $\\hat y_i = tanh(w.x_i)$. The loss function is given by: $\\sum_{i=1}^N l(y_i, \\hat y_i) + \\lambda \\|w\\|^2$ where $\\lambda$ is the weight of regularization. Derive the update rule for minimizing this loss using stochastic gradient descent with step size $\\eta$ when $l(y_i, \\hat y_i) = log_e(1 + exp(-y_i. \\hat y_i))$. In other words, at time $t+1$, express the new parameters $w_{t+1}$ in terms of the old parameters $w_t$. **[10 marks]**\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d8SHS4hq5UR"
   },
   "source": [
    "**Ans 1.1**\n",
    "For minimizing the loss using SGD, we need to find the gradient of the loss w.r.t $w$ i.e. $\\frac{\\partial L(y_{i}, \\hat y_{i})}{\\partial w}$. Here, $L(y_{i}, \\hat y_{i}) = l(y_{i}, \\hat y_{i}) + \\lambda ||w||^2 = \\log_{e}(1 + \\exp(-y_{i}.\\hat y_{i}))$.\n",
    "\n",
    "\n",
    "Now, $\\frac{\\partial L(y_{i}, \\hat y_{i})}{\\partial w} = 2 \\lambda w + \\frac{1}{1 + \\exp(-y_{i}.\\hat y_{i})} . \\exp(-y_{i}.\\hat y_{i}) . (-y_i). \\frac{\\partial \\hat y_{i}}{\\partial w}$\n",
    "\n",
    "$\\implies \\frac{\\partial L(y_{i}, \\hat y_{i})}{\\partial w} = 2 \\lambda w - \\frac{y_i}{1 + \\exp(y_{i}.\\hat y_{i})} . (1 - \\hat y_i^2) .x_i$\n",
    "\n",
    "$\\implies \\frac{\\partial L(y_{i}, \\hat y_{i})}{\\partial w} = 2 \\lambda w - \\frac{y_i . x_i}{1 + \\exp(y_{i}.\\hat y_{i})} . (1 - \\tanh^{2}(w.x_i))$\n",
    "\n",
    "From gradient descent rule with SGD, when instance $x_i$ was selected for training, we have\n",
    "$w_{t+1} = w_{t} - \\eta \\frac{\\partial L(y_{i}, \\hat y_{i})}{\\partial w_t}$\n",
    "\n",
    "$\\implies w_{t+1} = w_{t} - \\eta \\Big [  2 \\lambda w_t - \\frac{y_i . x_i}{1 + \\exp(y_{i}.\\hat y_{i})} . (1 - \\tanh^{2}(w_t.x_i)) \\Big ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrvD5wLZInKB"
   },
   "source": [
    "### Problem 2: Numerical Overflow and Underflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWVcuGP7CdMh"
   },
   "source": [
    "Run the cell below. Is the output the same as that you would expect? This is due to the condition which is called [numerical underflow](https://en.wikipedia.org/wiki/Arithmetic_underflow). It is the condition that occurs when the true result of a floating point operation is smaller in magnitude than the smallest value representable as a normal floating point number in the target datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vrtvkkddFCJG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e10+1e-10 == 1e10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LSxFiuOD0H9"
   },
   "source": [
    "Run the cell below. The warning message must have the word 'overflow' in it. This condition that occurs when a calculation produces a result that is greater in magnitude than the largest value representable in the target datatype is called [numerical overflow](https://en.wikipedia.org/wiki/Integer_overflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wwPzQe4TD1zZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-9a3a1acb84ee>:2: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(1000) == np.inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.exp(1000) == np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRUSbKitFLgl"
   },
   "source": [
    "**Q2.1**: How do people deal with numerical overflow and underflow? Why have we implemented $\\text{softplus}(x) = \\log(1+\\exp(x))$ as shown in the cell below? **[5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tVfDG7dfF1e6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softplus(x_, limit=5):\n",
    "    x = np.array(x_)\n",
    "    \n",
    "    # compute_real_mask is true => x lies between -limit and limit\n",
    "    compute_real_mask = np.logical_and(-limit < x, x < limit)\n",
    "    \n",
    "    # for x >= limit, ln(1 + exp(x)) = x\n",
    "    # store this in return_same_mask\n",
    "    return_same_mask = x >= limit \n",
    "    \n",
    "    # make the value as 0 for all the x not in the range (-limit, limit)\n",
    "    computed_real_part = np.log(1 + np.exp(x*compute_real_mask)*compute_real_mask)\n",
    "    \n",
    "    # add the input value for all the x >= limit [they are obviously not equal to zero!]\n",
    "    # return_same_mask is used here\n",
    "    returned_same_part = x*return_same_mask\n",
    "    return computed_real_part + returned_same_part\n",
    "\n",
    "def test_softplus():\n",
    "    x_arr = np.linspace(-200, 200)\n",
    "    softplus_true = np.log(1 + np.exp(x_arr))\n",
    "    softplus_stable = softplus(x_arr)\n",
    "    # assert the maximum error between the softplus_true and softplus_stable to be within 1e-3\n",
    "    assert np.max(abs(softplus_true-softplus_stable)) < 1e-3\n",
    "    \n",
    "test_softplus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJrAwp1CF1e8"
   },
   "source": [
    "**Ans 2.1** \n",
    "\n",
    "Methods to deal with numerical overflow and underflow:\n",
    "- Output a token (NaN - not a number) and interrupt the execution\n",
    "- Limit the user inputs over a range, presumably in which neither overflow nor underflow occurs\n",
    "- In case of underflow, clip the value to zero when the output is very very small and close to zero (implemented above in `soft_plus()`)\n",
    "- in case of overflow, clip the value to a particular value which is close to the actual value (implemented above in `soft_plus()`)\n",
    "- In case of overflow, if there are terms with $\\exp$, use their $\\log$ value to ensure less chances of overflow.\n",
    "\n",
    "\n",
    "The above code has been documented to mention the importance of each relevant section/line of code. The above implementation ensures that there is no underflow or overflow as the value of input to `softplus()` decreases to a large negative value or increases to a large positive value. Additionally, it also ensures that for any input `x` to `softplus()`, the output produced is within the error value of `1e-3` from the true value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcgYjUKKInKF"
   },
   "source": [
    "### Problem 3: Perceptron Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fjlfmcp1InKI"
   },
   "source": [
    "**Q3.1**: The code below generates $n$ 2D data points according to the Gaussian distribution $X1 \\sim \\mathcal{N}([1,0],\\,I_{2\\times2})$ and assigns them label 1. It also generates another $n$ 2D data points according to the Gaussian distribution $X2 \\sim \\mathcal{N}([-1,0],\\,I_{2\\times2})$ and assigns them label -1.\n",
    "\n",
    "If perceptron learning algorithm is used to classify the data, will it converge? You may add a code cell to answer this question.   **[5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "4MA0OGWYInKI"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########### commented portion can be used for visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "def generate_data(n):\n",
    "    '''Generate synthetic data'''\n",
    "    \n",
    "    X1 = np.random.multivariate_normal([1,0], [[1,0],[0,1]], n)\n",
    "    X2 = np.random.multivariate_normal([-1,0], [[1,0],[0,1]], n)\n",
    "    X = np.vstack((X1, X2))\n",
    "\n",
    "    y1 = np.ones(n, dtype=int)\n",
    "    y2 = -np.ones(n, dtype=int)\n",
    "    y = np.concatenate((y1, y2))\n",
    "\n",
    "#     blueX = [x[0] for x in X[:int(n)]]\n",
    "#     blueY = [x[1] for x in X[:int(n)]]\n",
    "#     orangeX = [x[0] for x in X[int(n) + 1:]]\n",
    "#     orangeY = [x[1] for x in X[int(n) + 1:]]\n",
    "#     plt.scatter(blueX, blueY);\n",
    "#     plt.scatter(orangeX, orangeY)\n",
    "    \n",
    "    \n",
    "    return X, y\n",
    "X_train, y_train = generate_data(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7292YPWIHMA"
   },
   "source": [
    "**Ans 3.1**\n",
    "- Perceptron learning algorithm converges only when there exists a hyperplane which correctly classifies all the given instances. Even when only a single instance is not correctly classified (i.e. mis-classified), the perceptron learning algorithm will continue to modify the parameters of the hyperplane.\n",
    "\n",
    "- In `generate_data()`, instances with label = 1, have a mean of 1 in distribution along x-axis and mean of 0 in the distribution along y-axis. Instances with label = -1, have a mean of -1 in distribution along x-axis and mean of 0 in the distribution along y-axis. This can be verified with uncommenting the commented portion above. Thus intuition speaks that `x = 0` would be the hyperplane that might classify all the instances properly.\n",
    "\n",
    "- Given the covariance matrix, there exists a probabilty of instances with label 1 to have abscissa less than 0 i.e. $[P(x < 0) > 0]$ for a normal distribution with mean value 1. Similarly, there exists a probabilty of instances with label -1 to have abscissa greater than 0 i.e. $[P(x > 0) > 0]$ for a normal distribution with mean value -1. Thus (generally) there would never exist a hyperplane that can classify all the instances properly. Hence, the Percetron learning algorithm never converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icMuAi-Lr1CO"
   },
   "source": [
    "### Problem 4: Implementing Multilayer Perceptron and Backpropagation   **[30 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZFH3r5liLSz"
   },
   "source": [
    "In this problem, we will be using [MNIST dataset](http://yann.lecun.com/exdb/mnist/) of handwritten digits for classification task. A data folder that will be useful for doing this problem has been provided to you along with this notebook.\n",
    "\n",
    "Let us load the data first (it has been done for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UKO-c29diLS0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Number of test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "## Load the training data from the data folder\n",
    "output_dim = 10\n",
    "x_train = np.load('./data/X_train.npy')\n",
    "x_train = x_train.flatten().reshape(-1,28*28)\n",
    "x_train = x_train / 255.0\n",
    "x_train = x_train.T\n",
    "gt_indices = np.load('./data/y_train.npy')\n",
    "train_length = x_train.shape[1]\n",
    "\n",
    "#creating one hot vector representation\n",
    "y_train = np.zeros((train_length, output_dim))\n",
    "for i in range(train_length):\n",
    "    y_train[i,gt_indices[i]] = 1\n",
    "y_train = y_train.T\n",
    "print(\"Number of training examples: {:d}\".format(train_length))\n",
    "\n",
    "## Loading the test data\n",
    "x_test = np.load('./data/X_test.npy')\n",
    "x_test = x_test.flatten().reshape(-1,28*28)\n",
    "x_test = x_test / 255.0\n",
    "x_test = x_test.T\n",
    "gt_indices = np.load('./data/y_test.npy')\n",
    "test_length = x_test.shape[1]\n",
    "\n",
    "#creating one hot vector representation\n",
    "y_test = np.zeros((test_length, output_dim))\n",
    "for i in range(test_length):\n",
    "    y_test[i,gt_indices[i]] = 1\n",
    "y_test = y_test.T\n",
    "print(\"Number of test examples: {:d}\".format(test_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cgce6j2MiLS1"
   },
   "source": [
    "This problem considers neural networks with multiple layers. Each layer has multiple inputs and outputs, and can be broken down into two parts:  \n",
    "\n",
    "A linear module that implements a linear transformation:     $ z_j = (\\sum^{m}_{i=1} x_i W_{i,j}) + {W_0}_j$  \n",
    "specified by a weight matrix $W$ and a bias vector $W_0$. The output is $[z_1, \\ldots, z_n]^T$\n",
    "\n",
    "An activation module that applies an activation function to the outputs of the linear module for some activation function $f$, such as Tanh or ReLU in the hidden layers or Softmax at the output layer. We write the output as: $[f(z_1), \\ldots, f(z_m)]^T$, although technically, for some activation functions such as softmax, each output will depend on all the $z_i$.\n",
    "\n",
    "We will use the following notation for quantities in a network:\n",
    "- Inputs to the network are $x_1,..., x_d$\n",
    "- Number of layers is $L$\n",
    "- There are $m^l$ inputs to layer $l$\n",
    "- There are $n^l = m^{l+1}$ outputs from layer $l$\n",
    "- The weight matrix for layer $l$ is $W^l$, an $m^l \\times n^l$ matrix, and the bias vector (offset) is $W_0^l$, an $n^l \\times 1$ vector\n",
    "- The outputs of the linear module for layer $l$ are known as pre-activation values and denoted $z^l$\n",
    "- The activation function at layer $l$ is $f^l(\\cdot)$\n",
    "- Layer $l$ activations are $a^l = [f^l(z^l_1), \\ldots, f^l(z^l_{n^l})]^T$\n",
    "- The output of the network is the values $a^L = [f^L(z^L_1), \\ldots, f^L(z^L_{n^L})]^T$\n",
    "- Loss function $Loss(a,y)$ measures the loss of output values $a$ when the target is $y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRIPVRhviLS1"
   },
   "source": [
    "We'll use the modular implementation, which leads to clean code. The basic framework for SGD training is given below. We can construct a network and train it as follows:\n",
    "\n",
    "```\n",
    "# build a 3-layer network\n",
    "net = Sequential([Linear(2,3), Tanh(),\n",
    "                  Linear(3,3), Tanh(),\n",
    "    \t          Linear(3,2), SoftMax()])\n",
    "# train the network on data and labels\n",
    "net.sgd(x_train, y_train)\n",
    "```\n",
    "Please fill in any unimplemented methods below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cY3OEZWRiLS1"
   },
   "source": [
    "## Linear Modules: ##\n",
    "Each linear module has a forward method that takes in a batch of activations A (from the previous layer) and returns a batch of pre-activations Z; ; it can also store its input or output vectors for use by other methods (e.g., for subsequent backpropagation).\n",
    "\n",
    "$Z = W^T A + W_0$\n",
    "\n",
    "Each linear module has a backward method that takes in a column vector dLdZ and returns dLdA. This module also computes and stores dLdW and dLdW0, the gradients with respect to the weights.\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial A} = \\frac{\\partial Z}{\\partial A} \\frac{\\partial Loss}{\\partial Z}$ and similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HENTHEIPiLS1"
   },
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def sgd_step(self, lrate): \n",
    "        pass # For modules without weights\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, m, n):\n",
    "        self.m, self.n = (m, n)  # (in size, out size)\n",
    "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
    "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Linear{}'.format((self.m, self.n))\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.A = A                              # (m x b)  b is the batch size, which is 1 for SGD\n",
    "        Z = self.W.T @ self.A + self.W0           \n",
    "        return Z  ## ! -- code required # (n x b)\n",
    "\n",
    "    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
    "        ## ! -- code required\n",
    "        self.dLdW = self.A @ dLdZ.T                         # (m x n)\n",
    "        self.dLdW0 = np.sum(dLdZ, axis=1, keepdims=True)    # (n x 1)\n",
    "        dLdA = self.W @ dLdZ                                # (m x n) . (n x b)\n",
    "        return dLdA                                         # return dLdA (m x b)\n",
    "\n",
    "    def sgd_step(self, lrate):  # Gradient descent step\n",
    "        ## ! -- code required\n",
    "        self.W = self.W - lrate * self.dLdW           \n",
    "        self.W0 = self.W0 - lrate * self.dLdW0           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_na1Jr-siLS2"
   },
   "source": [
    "## Activation functions: ##\n",
    "Activation modules don't have any weights and so they are simpler.\n",
    "\n",
    "Each activation module has a forward method that takes in a batch of pre-activations Z and returns a batch of activations A.\n",
    "\n",
    "Each activation module has a backward method that takes in dLdA and returns dLdZ, with the exception of SoftMax, where we assume dLdZ is passed in.\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial Z} = \\frac{\\partial Loss}{\\partial A} \\frac{\\partial A}{\\partial Z}$\n",
    "\n",
    "For Softmax = $SM(Z)$ at the output layer and cross entropy as the $Loss(A,Y)$ function, there is a [simple form](https://peterroelants.github.io/posts/cross-entropy-softmax/) for ${\\tt dLdZ} = \\frac{\\partial Loss}{\\partial Z}$; namely, it is the prediction error $Aâˆ’Y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bY3HFGcHCkj"
   },
   "source": [
    "### Tanh: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dFD9yB38iLS3"
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):            # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.tanh(Z)\n",
    "        return self.A\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Tanh()'\n",
    "    \n",
    "    def backward(self, dLdA):    # Uses stored self.A\n",
    "        return dLdA * (1 - self.A**2)  ## ! -- code required # return dLdZ (?, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkBAcTbyiLS3"
   },
   "source": [
    "### ReLU: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WNZgbM1ViLS3"
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):              # Layer activation\n",
    "    def forward(self, Z):\n",
    "        mask = (Z > 0)   ## ! -- code required # (?, b)\n",
    "        self.A = mask * Z\n",
    "        return self.A\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'ReLU()'\n",
    "    \n",
    "    def backward(self, dLdA):    # uses stored self.A\n",
    "        mask = np.array(self.A > 0).astype(int)\n",
    "        return mask * dLdA ## ! -- code required # return dLdZ (?, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qVlW8SFiLS3"
   },
   "source": [
    "### SoftMax: ###\n",
    "For `SoftMax.class_fun()`, given the column vector of class probabilities for each point (computed by Softmax), return a vector of the classes (integers) with the highest probability for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6KrMWcgwiLS4"
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):           # Output activation\n",
    "    def forward(self, Z):\n",
    "        A = np.exp(Z)/np.sum(np.exp(Z), axis=0)\n",
    "        return A  ## ! -- code required # (?, b)\n",
    "\n",
    "    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n",
    "        return dLdZ\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'SoftMax()'\n",
    "    \n",
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        class_indices = np.argmax(Ypred, axis=0)\n",
    "        return class_indices  ## ! -- code required # (1, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzwrK1XsiLS4"
   },
   "source": [
    "## Loss Function:##\n",
    "Each loss module has a forward method that takes in a batch of predictions Ypred (from the previous layer) and labels Y and returns a scalar loss value.\n",
    "\n",
    "The CrossE module has a backward method that returns dLdZ, the gradient with respect to the preactivation to SoftMax (note: not the activation!), since we are always pairing SoftMax activation with Cross Entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTp5-_H6iLS5"
   },
   "source": [
    "### Cross Entropy: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "E3t9-M8diLS5"
   },
   "outputs": [],
   "source": [
    "class CROSSE(Module):       # Loss\n",
    "    def forward(self, Ypred, Y):\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "        loss = -np.sum(np.multiply(self.Y, np.log(self.Ypred)), axis=0)\n",
    "        return loss  ## ! -- code required\n",
    "\n",
    "    def backward(self):  # Use stored self.Ypred, self.Y\n",
    "        dLdZ = self.Ypred - self.Y\n",
    "        return dLdZ  ## ! -- code required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XrtkyTNiLS5"
   },
   "source": [
    "## Neural Network: ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zFNsXNmiLS6"
   },
   "source": [
    "Implement SGD. Randomly pick a data point Xt, Yt by using np.random.randint to choose a random index into the data. Compute the predicted output Ypred for Xt with the forward method. Compute the loss for Ypred relative to Yt. Use the backward method to compute the gradients. Use the sgd_step method to change the weights. Repeat.\n",
    "\n",
    "Also, record the training accuracy after every 1000 iterations and plot it to show how the training accuracy changes with the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JSPrY1tViLS6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, modules, loss):            # List of modules, loss module\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '{}'.format(self.modules)\n",
    "    \n",
    "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
    "        D, N = X.shape\n",
    "        accuracies = [] # for plotting\n",
    "        itrns = [] # for plotting\n",
    "        for it in range(iters):\n",
    "            ## ! -- code required\n",
    "            i = np.random.randint(N)\n",
    "            Xt = X.T[i].reshape(-1,1)\n",
    "            Xt = self.forward(Xt)\n",
    "            \n",
    "            Yt = Y.T[i].reshape(-1,1)\n",
    "            loss = self.loss.forward(Xt, Yt)\n",
    "            delta = self.loss.backward()\n",
    "            self.backward(delta)\n",
    "            self.sgd_step(lrate)\n",
    "            if it % 1000 == 0 or it == iters-1:\n",
    "                acc = self.get_accuracy(X, Y)\n",
    "                accuracies.append(acc * 100)\n",
    "                itrns.append(it)\n",
    "                print('Iteration =', it, '\\tTraining Accuracy = %.2f%%' % (acc * 100))\n",
    "        ## ! -- code required\n",
    "        # plot accuracy vs iteration with appropriate labelling\n",
    "        plt.plot(itrns, accuracies, color='green')\n",
    "        plt.title('Accuracy vs Iterations')\n",
    "        plt.ylabel('Accuracy (in %)')\n",
    "        plt.xlabel('Iteration No.')\n",
    "\n",
    "    def forward(self, Xt):                        # Compute Ypred\n",
    "        for m in self.modules: \n",
    "            Xt = m.forward(Xt)\n",
    "        return Xt\n",
    "\n",
    "    def backward(self, delta):                    # Update dLdW and dLdW0\n",
    "        # Note reversed list of modules\n",
    "        for m in self.modules[::-1]: \n",
    "            delta = m.backward(delta)\n",
    "\n",
    "    def sgd_step(self, lrate):                    # Gradient descent step\n",
    "        for m in self.modules: \n",
    "            m.sgd_step(lrate)\n",
    "\n",
    "    def get_accuracy(self, X, Y):\n",
    "        # Method to print accuracy\n",
    "        cf = self.modules[-1].class_fun\n",
    "        acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OHGyeyJd1fu"
   },
   "source": [
    "Now, keeping the **number of hidden layers fixed at 2** and **learning rate fixed at 0.005** and the **number of iterations fixed at 17000**, try tuning the number of hidden units in each hidden layer as well as the activation function after every linear module. One example of a network having 30 hidden units in the first hidden layer followed by RELU activation and 20 in the second followed by Tanh activation is `nn = Sequential([Linear(input_dim, 30), ReLU(), Linear(30, 20), Tanh(), Linear(20,output_dim), SoftMax()], CROSSE())` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 0 \tTraining Accuracy = 9.15%\n",
      "Iteration = 1000 \tTraining Accuracy = 72.69%\n",
      "Iteration = 2000 \tTraining Accuracy = 83.17%\n",
      "Iteration = 3000 \tTraining Accuracy = 85.08%\n",
      "Iteration = 4000 \tTraining Accuracy = 86.27%\n",
      "Iteration = 5000 \tTraining Accuracy = 86.82%\n",
      "Iteration = 6000 \tTraining Accuracy = 88.17%\n",
      "Iteration = 7000 \tTraining Accuracy = 89.14%\n",
      "Iteration = 8000 \tTraining Accuracy = 90.08%\n",
      "Iteration = 9000 \tTraining Accuracy = 88.54%\n",
      "Iteration = 10000 \tTraining Accuracy = 90.32%\n",
      "Iteration = 11000 \tTraining Accuracy = 90.62%\n",
      "Iteration = 12000 \tTraining Accuracy = 89.95%\n",
      "Iteration = 13000 \tTraining Accuracy = 90.69%\n",
      "Iteration = 14000 \tTraining Accuracy = 90.94%\n",
      "Iteration = 15000 \tTraining Accuracy = 91.62%\n",
      "Iteration = 16000 \tTraining Accuracy = 92.04%\n",
      "Iteration = 16999 \tTraining Accuracy = 92.30%\n",
      "Test Acc = 0.9247\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_dim = 784  # input dimension\n",
    "nn = Sequential([Linear(input_dim, 30), ReLU(), Linear(30, 20), Tanh(), Linear(20,output_dim), SoftMax()], CROSSE()) ## ! -- code required\n",
    "nn.sgd(x_train, y_train, iters=17000, lrate=0.005)\n",
    "test_acc = nn.get_accuracy(x_test, y_test)\n",
    "print('Test Acc =', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning to a better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc = 0.9136, Model = [Linear(784, 32), ReLU(), Linear(32, 16), ReLU(), Linear(16, 10), SoftMax()]\n",
      "Test Acc = 0.911, Model = [Linear(784, 32), ReLU(), Linear(32, 16), Tanh(), Linear(16, 10), SoftMax()]\n",
      "Test Acc = 0.9157, Model = [Linear(784, 32), Tanh(), Linear(32, 16), ReLU(), Linear(16, 10), SoftMax()]\n",
      "Test Acc = 0.8936, Model = [Linear(784, 32), Tanh(), Linear(32, 16), Tanh(), Linear(16, 10), SoftMax()]\n",
      "Test Acc = 0.9205, Model = [Linear(784, 32), ReLU(), Linear(32, 32), ReLU(), Linear(32, 10), SoftMax()]\n",
      "Test Acc = 0.9183, Model = [Linear(784, 32), ReLU(), Linear(32, 32), Tanh(), Linear(32, 10), SoftMax()]\n",
      "Test Acc = 0.915, Model = [Linear(784, 32), Tanh(), Linear(32, 32), ReLU(), Linear(32, 10), SoftMax()]\n",
      "Test Acc = 0.9241, Model = [Linear(784, 32), Tanh(), Linear(32, 32), Tanh(), Linear(32, 10), SoftMax()]\n",
      "Test Acc = 0.918, Model = [Linear(784, 32), ReLU(), Linear(32, 64), ReLU(), Linear(64, 10), SoftMax()]\n",
      "Test Acc = 0.9251, Model = [Linear(784, 32), ReLU(), Linear(32, 64), Tanh(), Linear(64, 10), SoftMax()]\n",
      "Test Acc = 0.8924, Model = [Linear(784, 32), Tanh(), Linear(32, 64), ReLU(), Linear(64, 10), SoftMax()]\n",
      "Test Acc = 0.9191, Model = [Linear(784, 32), Tanh(), Linear(32, 64), Tanh(), Linear(64, 10), SoftMax()]\n",
      "Test Acc = 0.9185, Model = [Linear(784, 32), ReLU(), Linear(32, 128), ReLU(), Linear(128, 10), SoftMax()]\n",
      "Test Acc = 0.9214, Model = [Linear(784, 32), ReLU(), Linear(32, 128), Tanh(), Linear(128, 10), SoftMax()]\n",
      "Test Acc = 0.9225, Model = [Linear(784, 32), Tanh(), Linear(32, 128), ReLU(), Linear(128, 10), SoftMax()]\n",
      "Test Acc = 0.9222, Model = [Linear(784, 32), Tanh(), Linear(32, 128), Tanh(), Linear(128, 10), SoftMax()]\n",
      "Test Acc = 0.9185, Model = [Linear(784, 64), ReLU(), Linear(64, 16), ReLU(), Linear(16, 10), SoftMax()]\n",
      "Test Acc = 0.929, Model = [Linear(784, 64), ReLU(), Linear(64, 16), Tanh(), Linear(16, 10), SoftMax()]\n",
      "Test Acc = 0.9217, Model = [Linear(784, 64), Tanh(), Linear(64, 16), ReLU(), Linear(16, 10), SoftMax()]\n",
      "Test Acc = 0.9184, Model = [Linear(784, 64), Tanh(), Linear(64, 16), Tanh(), Linear(16, 10), SoftMax()]\n",
      "Test Acc = 0.9115, Model = [Linear(784, 64), ReLU(), Linear(64, 32), ReLU(), Linear(32, 10), SoftMax()]\n",
      "Test Acc = 0.9273, Model = [Linear(784, 64), ReLU(), Linear(64, 32), Tanh(), Linear(32, 10), SoftMax()]\n",
      "Test Acc = 0.9096, Model = [Linear(784, 64), Tanh(), Linear(64, 32), ReLU(), Linear(32, 10), SoftMax()]\n",
      "Test Acc = 0.915, Model = [Linear(784, 64), Tanh(), Linear(64, 32), Tanh(), Linear(32, 10), SoftMax()]\n",
      "Test Acc = 0.9277, Model = [Linear(784, 64), ReLU(), Linear(64, 64), ReLU(), Linear(64, 10), SoftMax()]\n",
      "Test Acc = 0.9188, Model = [Linear(784, 64), ReLU(), Linear(64, 64), Tanh(), Linear(64, 10), SoftMax()]\n",
      "Test Acc = 0.927, Model = [Linear(784, 64), Tanh(), Linear(64, 64), ReLU(), Linear(64, 10), SoftMax()]\n",
      "Test Acc = 0.9225, Model = [Linear(784, 64), Tanh(), Linear(64, 64), Tanh(), Linear(64, 10), SoftMax()]\n",
      "Test Acc = 0.9224, Model = [Linear(784, 64), ReLU(), Linear(64, 128), ReLU(), Linear(128, 10), SoftMax()]\n",
      "Test Acc = 0.9278, Model = [Linear(784, 64), ReLU(), Linear(64, 128), Tanh(), Linear(128, 10), SoftMax()]\n",
      "Test Acc = 0.9242, Model = [Linear(784, 64), Tanh(), Linear(64, 128), ReLU(), Linear(128, 10), SoftMax()]\n",
      "Test Acc = 0.9175, Model = [Linear(784, 64), Tanh(), Linear(64, 128), Tanh(), Linear(128, 10), SoftMax()]\n",
      "Test Acc = 0.9234, Model = [Linear(784, 128), ReLU(), Linear(128, 16), ReLU(), Linear(16, 10), SoftMax()]\n",
      "Test Acc = 0.9262, Model = [Linear(784, 128), ReLU(), Linear(128, 16), Tanh(), Linear(16, 10), SoftMax()]\n",
      "Test Acc = 0.916, Model = [Linear(784, 128), Tanh(), Linear(128, 16), ReLU(), Linear(16, 10), SoftMax()]\n",
      "Test Acc = 0.9112, Model = [Linear(784, 128), Tanh(), Linear(128, 16), Tanh(), Linear(16, 10), SoftMax()]\n",
      "Test Acc = 0.93, Model = [Linear(784, 128), ReLU(), Linear(128, 32), ReLU(), Linear(32, 10), SoftMax()]\n",
      "Test Acc = 0.9275, Model = [Linear(784, 128), ReLU(), Linear(128, 32), Tanh(), Linear(32, 10), SoftMax()]\n",
      "Test Acc = 0.9199, Model = [Linear(784, 128), Tanh(), Linear(128, 32), ReLU(), Linear(32, 10), SoftMax()]\n",
      "Test Acc = 0.916, Model = [Linear(784, 128), Tanh(), Linear(128, 32), Tanh(), Linear(32, 10), SoftMax()]\n",
      "Test Acc = 0.9329, Model = [Linear(784, 128), ReLU(), Linear(128, 64), ReLU(), Linear(64, 10), SoftMax()]\n",
      "Test Acc = 0.9307, Model = [Linear(784, 128), ReLU(), Linear(128, 64), Tanh(), Linear(64, 10), SoftMax()]\n",
      "Test Acc = 0.9186, Model = [Linear(784, 128), Tanh(), Linear(128, 64), ReLU(), Linear(64, 10), SoftMax()]\n",
      "Test Acc = 0.9227, Model = [Linear(784, 128), Tanh(), Linear(128, 64), Tanh(), Linear(64, 10), SoftMax()]\n",
      "Test Acc = 0.9232, Model = [Linear(784, 128), ReLU(), Linear(128, 128), ReLU(), Linear(128, 10), SoftMax()]\n",
      "Test Acc = 0.9217, Model = [Linear(784, 128), ReLU(), Linear(128, 128), Tanh(), Linear(128, 10), SoftMax()]\n",
      "Test Acc = 0.9302, Model = [Linear(784, 128), Tanh(), Linear(128, 128), ReLU(), Linear(128, 10), SoftMax()]\n",
      "Test Acc = 0.9172, Model = [Linear(784, 128), Tanh(), Linear(128, 128), Tanh(), Linear(128, 10), SoftMax()]\n",
      "Test Acc = 0.9306, Model = [Linear(784, 256), ReLU(), Linear(256, 16), ReLU(), Linear(16, 10), SoftMax()]\n",
      "Test Acc = 0.9353, Model = [Linear(784, 256), ReLU(), Linear(256, 16), Tanh(), Linear(16, 10), SoftMax()]\n",
      "Test Acc = 0.8993, Model = [Linear(784, 256), Tanh(), Linear(256, 16), ReLU(), Linear(16, 10), SoftMax()]\n",
      "Test Acc = 0.9028, Model = [Linear(784, 256), Tanh(), Linear(256, 16), Tanh(), Linear(16, 10), SoftMax()]\n",
      "Test Acc = 0.9216, Model = [Linear(784, 256), ReLU(), Linear(256, 32), ReLU(), Linear(32, 10), SoftMax()]\n",
      "Test Acc = 0.9358, Model = [Linear(784, 256), ReLU(), Linear(256, 32), Tanh(), Linear(32, 10), SoftMax()]\n",
      "Test Acc = 0.9172, Model = [Linear(784, 256), Tanh(), Linear(256, 32), ReLU(), Linear(32, 10), SoftMax()]\n",
      "Test Acc = 0.9148, Model = [Linear(784, 256), Tanh(), Linear(256, 32), Tanh(), Linear(32, 10), SoftMax()]\n",
      "Test Acc = 0.9249, Model = [Linear(784, 256), ReLU(), Linear(256, 64), ReLU(), Linear(64, 10), SoftMax()]\n",
      "Test Acc = 0.9338, Model = [Linear(784, 256), ReLU(), Linear(256, 64), Tanh(), Linear(64, 10), SoftMax()]\n",
      "Test Acc = 0.9144, Model = [Linear(784, 256), Tanh(), Linear(256, 64), ReLU(), Linear(64, 10), SoftMax()]\n",
      "Test Acc = 0.9193, Model = [Linear(784, 256), Tanh(), Linear(256, 64), Tanh(), Linear(64, 10), SoftMax()]\n",
      "Test Acc = 0.936, Model = [Linear(784, 256), ReLU(), Linear(256, 128), ReLU(), Linear(128, 10), SoftMax()]\n",
      "Test Acc = 0.9382, Model = [Linear(784, 256), ReLU(), Linear(256, 128), Tanh(), Linear(128, 10), SoftMax()]\n",
      "Test Acc = 0.925, Model = [Linear(784, 256), Tanh(), Linear(256, 128), ReLU(), Linear(128, 10), SoftMax()]\n",
      "Test Acc = 0.9177, Model = [Linear(784, 256), Tanh(), Linear(256, 128), Tanh(), Linear(128, 10), SoftMax()]\n"
     ]
    }
   ],
   "source": [
    "acc = {}\n",
    "fhu = np.logspace(start=5, stop=8, num=4, base=2, dtype=np.int)\n",
    "shu = np.logspace(start=4, stop=7, num=4, base=2, dtype=np.int)\n",
    "input_dim, output_dim = 784, 10\n",
    "\n",
    "for first in fhu:\n",
    "    for second in shu:\n",
    "        for act_1 in [ReLU(), Tanh()]:\n",
    "            for act_2 in [ReLU(), Tanh()]:\n",
    "                nn = Sequential([Linear(input_dim, first), \n",
    "                                 act_1, Linear(first, second), \n",
    "                                 act_2, Linear(second, output_dim), \n",
    "                                 SoftMax()], \n",
    "                                CROSSE())  \n",
    "                nn.sgd(x_train, y_train, iters=17000, lrate=0.005)\n",
    "                test_acc = nn.get_accuracy(x_test, y_test)\n",
    "                acc[nn] = test_acc\n",
    "                print('Test Acc = {}, Model = {}'.format(test_acc, nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the better model architecture and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Test Accuracy:  0.9382\n",
      "Model:  [Linear(784, 256), ReLU(), Linear(256, 128), Tanh(), Linear(128, 10), SoftMax()]\n"
     ]
    }
   ],
   "source": [
    "test_acc_values = np.array(list(acc.values()))\n",
    "maximum_test_acc = np.max(test_acc_values)\n",
    "for nn, acc in list(acc.items()):\n",
    "    if acc == maximum_test_acc:\n",
    "        print('Highest Test Accuracy: ', acc)\n",
    "        print('Model: ', nn)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmJ4TaVBF1fL"
   },
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "Wow, you made it this far. That is impressive. Congratulations!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Questions_Problem_Set_02.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
